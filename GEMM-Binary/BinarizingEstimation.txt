DNNの様々な検出傾向について、それぞれのDNNをバイナリ化した場合の影響

ネットワークはGivenとした場合、バイナリ化することにより、低下する認識性能と向上する認識速度は予測されていない

バイナリ化することによる認識性能の低下を避けるため、高度なDNNに設計変更すれば、認識速度が落るので、認識性能と認識速度はトレードオフである

しかし、実際にどの程度認識性能が落ちるのかを知る方法やソリューションは見当たらないので、トレードオフには多大なコストがかかる
（今現在試行しているDNNの小型化やバイナリ化の作業はトレードオフを探る作業そのものだ）

仮に、このようなソリューションが存在した場合、
・ネットワーク構成を仮決めして、ベンチマークテスト
・バイナライズして、ベンチマークテスト
・ネットワーク設計変更
この反復になる

また、
・ベンチマークデータの確定
・クライテリアの確定
なども重要

そこで、めぼしいネットワークを列挙してバイナライズし、
ネットワーク  ベンチマーク  Float  バイナライズ(FPS/Precision)
  NN1           VOC     10/98      30/80
  NN2           VOC     11/88      22/80
...
などのテーブルを作るとする

これにより実用化すべきネットワーク性能が決まったら、そのネットワークに近いものをこのテーブルから探し、バイナライズの影響を予測することができる

ただし、ネットワークの種類は無数にあるので、特定のAI-FrameWorkだけでは対応しない層構造も多い

これらをバイナライズする方法もまた無数に存在する

Convolutional Layerのみバイナライズすることは、手始めの作業としては有力だ

認識精度の向上は、DNN研究者にとって至上命題であり、それをわざわざバイナライズで認識速度の犠牲にすることはない

今後もこの傾向は変わらず続くであろうが、今回のAIブームでは実用になりそうなものが出て来るであろう

つまり実用に耐えるための研究も進んでいるからである

今やるべきことは、AI FrameWorkの機能拡張である
・バイナライズをサポート
・ウェイト、イメージの選択的バイナライズと±1 bainary、±Normなど2値化の方法
・GPGPU、CPU、FPGAでのバイナライズの効果

現段階では、バイナライズにより認識速度が上がるのか下がるのかさえ測定できていない
2018.04.21

