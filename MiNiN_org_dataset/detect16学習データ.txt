人体検出用に教師信号と入力画像を作る
このメモは、画像を16分割し、人物が写る場所に全て1を立てた教師信号で学習するケース
誤りが多く、使えない

[■画像データ]
画像入力
32x32へresize
'train'と'test'のキーで各キーにそれぞれ50,000と10,000データ、
1データがR1024、G1024、B1024の並びでnumpyを作る
pickleでファイルセーブ

[■教師信号]
回帰問題として定義する
32x32入力画像を8x8の領域(が全部で16個)に分けて考える
教師信号の与え方に幾つかある

①8x8の各領域に対象物の一部があれば1、なければ0
　対象物がアップになっていると、対象物を含む8x8領域が多数できる
　一方、対象物が遠くにあるとき、対象物を含む8x8領域は減る
　対象物が複数ある場合、分離されない可能性がある
　対象物が複数に見えることに問題はないのか？

②8x8の各領域に対象物の一部があれば1/(対象物を含む領域数)、なければ0
　対象物がアップになっていると、対象物を含む8x8領域が多数できる
　一方、対象物が遠くにあるとき、対象物を含む8x8領域は減る
　誤差関数の出力値が相対的に小さくなり問題が簡単になる
　対象物が複数ある場合、分離されない可能性がある
　対象物が複数に見えることに問題はないのか？
　検出/未検出のスレッショルドが一定値にならないことに問題がある

③8x8の各領域に対象物の中心があれば1、なければ0
　対象物がアップになっていると、対象物の中心を含まず、一部のみ含む8x8領域が多数できる
　一方、対象物が遠くにあるとき、対象物の中心を含まず、対象物の一部のみ含む8x8領域は減る
　対象物が複数ある場合、分離される
　対象物の一部のみ含む領域でも、誤差が大きくなり問題に矛盾が起きないか？
　YOLOではλを導入して誤差を調整しているが、今回はSSEを使う予定

方法として①が有力

xmlからperson情報だけを含むtxtに変換
32x32を8x8領域に分けてbboxとの積領域があれば1,なければ0

[■VOCデータのannotation情報]
class(数字) x(box中心) y(box中心) w(*画像幅=pixels) h(*画像高=pixels)

$ cat data/voc.names  
aeroplane、bicycle               
bird     、boat                  
bottle   、bus                   
car      、cat                   
chair    、cow                   
diningtable   、dog                   
horse    、motorbike             
person（14が人)                
pottedplant   、sheep                 
sofa     、train                 
tvmonitor

[■VOCデータでの人の見つけ方]
なので、
class==14で、
int((x-w/2)/0.25) ~ int(((x-w/2)+w)/0.25) かつ int((y-h/2)/0.25) ~ int(((y-h/2)+h)/0.35)

例えば、
14 0.492 0.441441441441 0.108 0.24024024024
↓
for i == 1 ~ 2: 
    for j == 0 ~ 3:
        Truth[i][j] = 1.

と教師信号を生成できるので、元画像サイズに依存せず1.を立てられる

[■しかし小さく写っている人はresizeによってまるで見えなくなる可能性がある]
元画像サイズが大きい時、32x32へのresizeにより問題となる
4x4分割領域の縦横絶対値でも選択する
4x4分割領域の絶対サイズが3x3になったら、max_poolingのカーネルと同じサイズ

リサイズ率＝32/元画像サイズ
リサイズ後に1x1になる元画像の領域＝元画像サイズ/32
当然:-)

なので、
3 x 元画像サイズ/32
より小さい元画像の領域は3x3より小さくなる

例えば元画像224x224なら
3x224/32 = 21pixel
なので、21x21pixelより小さい領域はresizeにより3x3以下になり見えなくなるだろう

wとhに元画像サイズを掛けた大きさ >= 21pixel
であればresizeしても人として見える可能性がある

だが、
if class == 14 && w * 元画像サイズ >= 3 * 元画像サイズ / 32:
は両辺元画像サイズで割れるので、
↓
if class == 14 && w >= 3 / 32:
    for i == 1 ~ 2: 
        for j == 0 ~ 3:
            Truth[i][j] = 1.

となって、やっぱり元画像サイズに依存せず1.を立てられる

[■pickleでの書出し]
cifar10の学習検証データは、
test, trainの二つのキー
各キーに10,000と50,000のデータ
x['test'].shapeは(10000L,3072)  ※3072=32*32*3
pixelの並びはRGB
numpy.float32が各要素

ラベルデータは、
test, trainの二つのキー
各キーに10,000と50,000のデータ
labels['test'].shapeは(10000L,)
numpy.int32が各要素でクラス番号を示す

となっているので、同じフォーマットで書き出す

[■posi/nega/ambiの辞書キーに分けて1つのpickleファイルへ保存]
例えば、VOCデータセットをファイル名voc_ds.pklで保存するならば、
 辞書キーは、
 ['image_posi' 'image_nega' 'image_ambi' 'truth_posi' 'truth_nega' 'truth_ambi'
 'path_posi' 'path_nega' 'path_ambi' ]
イメージと真値を、ポジ、ネガ、あいまい、とそれぞれの画像ファイルパスを辞書形式で保存
キーは必ず9つ持ち、len(image['image_posi'])==0などもあり得る
イメージはデータ数x3x32x32、真値はデータ数x16、とする
※ambignuousはターゲットが写ってはいるが小さすぎて誤解を招く画像など

これらをVOC、LFW、indoorなどについて作成する
voc_ds.pkl
lfw_ds.pkl
indoor_ds.pkl
など
voc_reg16.py lfw_reg16.pyで作成可能

[■複数のpklファイルを再構成する]
全小領域合計で、ある：ない＝1：1とすること

これらのpickleファイルを用い、
Posi:Nega == 7:3(多分類問題)
Posi:Nega == 1:1(2分類問題)
などの経験則を加味した比率で学習データとテストデータに分けたpickleファイルに再構成する
※2分類問題でのposi/nega比率は画像枚数を基準に求めると誤る点に注意

①voc_ds.pklとlfw_ds.pklの全てのposiデータを読み込む　　：image_posiN
②voc_ds.pklとindoor_ds.pklの全てのネガデータを読み込む ：image_negaN
③image_posiN：image_negaN == 7 : 3
　または
③imageに含まれるposiN領域数:negaN領域数 == 1 : 1

として2つのファイルを構成する
※今回は領域数の比を1:1とした

image.pkl ＠keys()=['test', 'train']
truth.pkl ＠keys()=['test', 'train']
↑この形式はCifar10 DataSetの形式である

reconst_reg16.py
でpklファイルを作る

[■学習と結果]
学習はchainerV2.0.2+Linux+GTX1080で
87%前後の正解率

[■カメラ]
とっても間違いが目立つ
13%のエラー率で16領域では平均して2,3個所間違える計算だが、これが目立つ
っつーか、目立ち過ぎ
この結果ではデモなんかには使えない

[■次]
YOLOv2のロス関数をもう一度よーっく見て別案を検討する;-<
